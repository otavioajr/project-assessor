# ğŸ¤– IntegraÃ§Ã£o LLM - Assessor Financeiro

Esta documentaÃ§Ã£o explica como a LLM (Large Language Model) foi integrada ao Assessor Financeiro como fallback inteligente para o parser de mensagens.

## ğŸ“‹ VisÃ£o Geral

A LLM funciona como um **fallback** quando o parser tradicional baseado em regras nÃ£o consegue identificar o tipo de mensagem. Ela Ã© **pluggable** - vocÃª pode facilmente trocar entre diferentes provedores.

### ğŸ¯ Objetivos da IntegraÃ§Ã£o

- **NÃ£o alucinaÃ§Ã£o**: LLM restrita ao escopo do projeto
- **Pluggable**: FÃ¡cil troca entre provedores (Ollama, OpenAI, etc.)
- **Fallback**: Usado apenas quando parser tradicional falha
- **ConfianÃ§a limitada**: LLM nÃ£o pode ter mais confianÃ§a que parser tradicional

## ğŸ—ï¸ Arquitetura

```
Mensagem â†’ Parser Tradicional â†’ [Sucesso] â†’ Resposta
              â†“ [Falha]
           LLM Fallback â†’ Resposta
```

### ğŸ“ Estrutura de Arquivos

```
backend/src/nlp/llm/
â”œâ”€â”€ types.ts              # Interfaces e tipos
â”œâ”€â”€ base-provider.ts      # Classe abstrata base
â”œâ”€â”€ ollama-provider.ts    # ImplementaÃ§Ã£o Ollama
â”œâ”€â”€ disabled-provider.ts  # Provider desabilitado
â”œâ”€â”€ factory.ts           # Factory para providers
â””â”€â”€ index.ts             # Exports
```

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. VariÃ¡veis de Ambiente (.env)

```bash
# LLM Configuration
LLM_ENABLED=true
LLM_PROVIDER=ollama
LLM_BASE_URL=http://localhost:11434
LLM_MODEL=gemma2:2b
LLM_MAX_TOKENS=500
LLM_TEMPERATURE=0.1
```

### 2. Provedores DisponÃ­veis

- **ollama**: Local, gratuito, privado
- **openai**: API externa, pago
- **disabled**: Desabilitado (fallback para unknown)

## ğŸš€ Setup RÃ¡pido

### 1. Usando Docker (Recomendado)

```bash
# Subir todos os serviÃ§os incluindo Ollama
docker compose up -d

# Configurar Ollama e baixar modelo
./setup-ollama.sh

# Verificar logs
docker compose logs ollama
docker compose logs backend
```

### 2. Setup Manual do Ollama

```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Baixar modelo
ollama pull gemma2:2b

# Verificar
ollama list
```

## ğŸ“Š Como Funciona

### 1. Fluxo de Parsing

```typescript
parseMessage(text) â†’ {
  1. Verifica queries (saldo, resumo)
  2. Extrai valores monetÃ¡rios
  3. Identifica eventos com data/hora
  4. ğŸ¤– Fallback LLM se nada foi identificado
  5. Retorna 'unknown' se LLM tambÃ©m falhar
}
```

### 2. Prompt Estruturado

A LLM recebe um prompt cuidadosamente estruturado:

```
VocÃª Ã© um parser especializado para um Assessor Financeiro via WhatsApp.
ESCOPO DO PROJETO: [contexto detalhado]
CATEGORIAS DISPONÃVEIS: [lista de categorias]
INSTRUÃ‡Ã•ES: [regras anti-alucinaÃ§Ã£o]
MENSAGEM: "[texto do usuÃ¡rio]"
```

### 3. Resposta Estruturada

```json
{
  "type": "transaction|query|event|unknown",
  "confidence": 0.0-0.7,
  "reasoning": "explicaÃ§Ã£o",
  "data": { /* dados extraÃ­dos */ }
}
```

## ğŸ›¡ï¸ PrevenÃ§Ã£o de AlucinaÃ§Ã£o

### Medidas Implementadas

1. **Contexto Restrito**: LLM recebe apenas informaÃ§Ãµes do projeto
2. **Categorias Limitadas**: Lista fechada de categorias vÃ¡lidas
3. **ConfianÃ§a MÃ¡xima**: LLM limitada a 70% de confianÃ§a
4. **ValidaÃ§Ã£o Rigorosa**: Resposta validada antes de usar
5. **Fallback Seguro**: Se LLM falhar, retorna 'unknown'

### Prompt Anti-AlucinaÃ§Ã£o

```
INSTRUÃ‡Ã•ES RIGOROSAS:
1. NUNCA invente informaÃ§Ãµes que nÃ£o estÃ£o na mensagem
2. APENAS identifique se a mensagem se encaixa em uma das categorias
3. Se nÃ£o souber categorizar, responda type: "unknown"
4. Use apenas as categorias fornecidas
5. ConfianÃ§a mÃ¡xima permitida: 0.7
```

## ğŸ”„ Trocar de Provedor

### Para OpenAI

```bash
# .env
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-your-key-here
LLM_MODEL=gpt-3.5-turbo
```

### Para Desabilitar

```bash
# .env
LLM_ENABLED=false
# ou
LLM_PROVIDER=disabled
```

## ğŸ§ª Testes

### 1. Teste Manual via API

```bash
# Testar parser diretamente
curl -X POST http://localhost:3001/test-parser \
  -H "Content-Type: application/json" \
  -d '{"message": "comprei uma camiseta por 50 reais"}'
```

### 2. Teste via WhatsApp

Envie mensagens ambÃ­guas que o parser tradicional nÃ£o consegue processar:

- "gastei dinheiro no mercado"
- "recebi um valor hoje" 
- "preciso economizar mais"

### 3. Verificar Logs

```bash
docker compose logs backend | grep LLM
docker compose logs backend | grep Parser
```

## ğŸ“ˆ Monitoramento

### MÃ©tricas Importantes

- **Taxa de Fallback**: Quantas mensagens usam LLM
- **ConfianÃ§a MÃ©dia**: ConfianÃ§a das respostas da LLM  
- **Taxa de Unknown**: Mensagens nÃ£o identificadas
- **Tempo de Resposta**: LatÃªncia da LLM

### Logs Estruturados

```
[Parser] Usando LLM fallback para: "comprei algo"
[Ollama] âœ… DisponÃ­vel com modelo gemma2:2b
[Parser] LLM identificou: transaction (confianÃ§a: 0.6)
```

## ğŸš¨ Troubleshooting

### Ollama nÃ£o responde

```bash
# Verificar se estÃ¡ rodando
curl http://localhost:11434/api/version

# Reiniciar
docker compose restart ollama

# Ver logs
docker compose logs ollama
```

### Modelo nÃ£o encontrado

```bash
# Listar modelos
curl http://localhost:11434/api/tags

# Baixar modelo
./setup-ollama.sh
```

### LLM muito lenta

```bash
# Usar modelo menor
LLM_MODEL=gemma:2b

# Reduzir tokens
LLM_MAX_TOKENS=200
```

## ğŸ”’ Privacidade e SeguranÃ§a

### Ollama (Local)
- âœ… Dados ficam localmente
- âœ… Sem envio para internet
- âœ… Controle total dos dados

### OpenAI (Externa)  
- âš ï¸ Dados enviados para API externa
- âš ï¸ Sujeito aos termos da OpenAI
- âœ… Usar apenas com dados nÃ£o sensÃ­veis

## ğŸ“š PrÃ³ximos Passos

1. **MÃ©tricas**: Implementar coleta de mÃ©tricas de performance
2. **Cache**: Cache de respostas da LLM para mensagens similares
3. **Fine-tuning**: Treinar modelo especÃ­fico para o domÃ­nio
4. **A/B Testing**: Comparar performance com e sem LLM
5. **Outros Providers**: Suporte para Anthropic, Cohere, etc.
